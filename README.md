# ElasticTrain
A flexible compressed DNN training framework, capable of training robust DNNs while simultaneously satisfying a mixture of different constraints, including unstructured, structured, pattern-based, and a novel workload-balanced pruning, as well as multi-codebook quantization.

# Updates
We will soon submit a research paper under the title of "ElasticTrain: A Flexible, Ultra-High Compression DNN Training Framework", and we will open-source the framework after publication; however, some of our ultra-compressed models with the starter code are available here at the moment.

# Sample Models
The table below lists our trained models available here early, and summarises their compression details.
All the listed models can be found in `TrainedModels` directory.

# Testing the Models
A simple starter code is provided in `ModelEval.py` to assist with loading, testing, and exploring the compressed trained models.
